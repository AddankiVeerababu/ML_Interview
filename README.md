# ML_Interview
#Question 1:Suppose that we design a deep architecture to represent a sequence by stacking self-attention layers with positional encoding. What could be issues? (paragraph format)

When we build a deep model using many self-attention layers, we add positional encoding to help the model understand the order of the input sequence. However, there are several issues that can arise. One main problem is that as the input passes through multiple layers, the model may gradually lose track of position information, especially if we use fixed encodings like sine and cosine. These encodings do not adapt during training, so their impact may weaken in deeper layers, making it harder for the model to preserve the order of tokens.

Another issue is that attention layers, particularly in deeper parts of the model, can begin to focus on the same regions of the sequence. This reduces the diversity of information being learned across layers and limits the modelâ€™s ability to capture complex or hierarchical patterns. Additionally, if the positional encoding remains static and does not evolve with the learning process, the model may struggle to understand nuanced relationships in longer or more structured sequences.

To address these limitations, learnable positional encodings are often used. These allow the model to update how it understands position during training, leading to better flexibility and performance. However, learnable encodings introduce their own challenges. If not regularized properly, they can cause overfitting to specific sequence lengths or token positions seen during training, which reduces generalization. Since they are typically defined for a fixed maximum length, they may also struggle to handle variable-length or longer sequences effectively unless special care is taken during design.

Additionally, stacking many layers increases computational cost and memory usage, which can become inefficient when processing long sequences. In very deep networks, there is also a risk of gradient saturation through the positional channels, making it harder for the model to reinforce position-awareness across layers. These challenges highlight the importance of carefully designing how positional encoding is integrated into deep transformer architectures to ensure both order-awareness and training stability.
