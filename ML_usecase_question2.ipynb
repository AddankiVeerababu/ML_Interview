{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "9d4LCV4YnEdw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy dataset: generates random sequences of token IDs\n",
        "class DummySequenceDataset(Dataset):\n",
        "    def __init__(self, vocab_size=100, seq_len=10, num_samples=5):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.randint(0, self.vocab_size, (self.seq_len,))\n",
        "\n"
      ],
      "metadata": {
        "id": "0ApBfRZeo4qi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learnable positional encoding module\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.position_embedding[:, :seq_len, :]\n",
        "\n"
      ],
      "metadata": {
        "id": "-LNqp2Awo7Yc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal demo showing learnable positional encoding in action\n",
        "def run_demo():\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "    vocab_size = 100\n",
        "    d_model = 16\n",
        "\n",
        "    # Dummy token sequences\n",
        "    dataset = DummySequenceDataset(vocab_size=vocab_size, seq_len=seq_len)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Embedding + Positional Encoding\n",
        "    embedding = nn.Embedding(vocab_size, d_model)\n",
        "    pos_encoding = LearnablePositionalEncoding(max_len=seq_len, d_model=d_model)\n",
        "\n",
        "    for batch in loader:\n",
        "        print(\"Token IDs:\\n\", batch)\n",
        "        embedded = embedding(batch)  # [batch, seq_len, d_model]\n",
        "        print(\"\\nToken Embeddings Shape:\", embedded.shape)\n",
        "\n",
        "        output = pos_encoding(embedded)\n",
        "        print(\"\\nOutput After Adding Learnable Positional Encoding:\\n\", output[0, :, :4])  # show 4 dims of first sample\n",
        "        break\n",
        "\n",
        "run_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2anDxaWo9Nh",
        "outputId": "cfeef196-0dce-467d-b5ca-f2d79096ea29"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[23, 45,  2, 33, 22,  8, 32, 41, 45, 36],\n",
            "        [50, 71, 93, 65, 33, 68, 41, 16, 30, 61]])\n",
            "\n",
            "Token Embeddings Shape: torch.Size([2, 10, 16])\n",
            "\n",
            "Output After Adding Learnable Positional Encoding:\n",
            " tensor([[ 0.8761, -0.5855, -0.4641, -0.8456],\n",
            "        [ 0.5093, -0.3224, -0.2590, -2.6530],\n",
            "        [ 1.0682,  1.1623, -3.2055, -1.7464],\n",
            "        [ 1.1452, -1.7065,  0.2862, -0.2955],\n",
            "        [ 1.2297, -0.1780, -1.4761,  0.2236],\n",
            "        [-2.1803, -1.9979, -2.9829, -0.3825],\n",
            "        [ 0.7024,  1.8152,  0.8138, -0.0145],\n",
            "        [ 0.4441,  1.5403, -1.4557,  2.4941],\n",
            "        [ 2.1826, -2.1226, -0.2999,  1.3709],\n",
            "        [-1.8351,  0.4538,  2.1026,  0.8340]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    }
  ]
}